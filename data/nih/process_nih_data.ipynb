{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe107ecb-7b6d-4bda-b1ba-e348897ff9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bc44c03-c3b5-4409-a7c0-7f72bedac6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(s, p = 0.25):\n",
    "    return s if np.random.random() > p else 1 - s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f396dce4-5c53-4054-975b-d9be6935ed24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2t/tpzk6y551s5bvtpwm5msn6740000gn/T/ipykernel_1019/3549364345.py:19: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  df['consensus'] = df['Y_H'].apply(lambda x: stats.mode(x)[0][0])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expert1</th>\n",
       "      <th>expert2</th>\n",
       "      <th>expert3</th>\n",
       "      <th>expert4</th>\n",
       "      <th>expert5</th>\n",
       "      <th>img_name</th>\n",
       "      <th>model_p0</th>\n",
       "      <th>model_p1</th>\n",
       "      <th>model_pred_noisy</th>\n",
       "      <th>Y_H</th>\n",
       "      <th>consensus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>00000211_006.png</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.999332</td>\n",
       "      <td>0.999332</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>00000211_022.png</td>\n",
       "      <td>0.013090</td>\n",
       "      <td>0.986910</td>\n",
       "      <td>0.986910</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>00000211_028.png</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>0.993304</td>\n",
       "      <td>0.993304</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>00000211_029.png</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.999482</td>\n",
       "      <td>0.999482</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>00000218_002.png</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.999544</td>\n",
       "      <td>0.999544</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   expert1  expert2  expert3  expert4  expert5          img_name  model_p0  \\\n",
       "0        1        1        1        1        1  00000211_006.png  0.000668   \n",
       "1        1        1        1        1        1  00000211_022.png  0.013090   \n",
       "2        1        1        1        1        1  00000211_028.png  0.006696   \n",
       "3        1        1        1        1        1  00000211_029.png  0.000518   \n",
       "4        1        1        1        1        1  00000218_002.png  0.000456   \n",
       "\n",
       "   model_p1  model_pred_noisy              Y_H  consensus  \n",
       "0  0.999332          0.999332  [1, 1, 1, 1, 1]          1  \n",
       "1  0.986910          0.986910  [1, 1, 1, 1, 1]          1  \n",
       "2  0.993304          0.993304  [1, 1, 1, 1, 1]          1  \n",
       "3  0.999482          0.999482  [1, 1, 1, 1, 1]          1  \n",
       "4  0.999544          0.999544  [1, 1, 1, 1, 1]          1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create combined dataframe of expert + model predictions\n",
    "human_label_path = 'nih_full_raw_data/all_findings_expert_labels/test_individual_readers.csv'\n",
    "n_experts = 5\n",
    "annotator_id_cols = [4343880271, 4343882583, 4343882785, 4343883593, 4343883996]\n",
    "human_labels = pd.read_csv(human_label_path)\n",
    "human_labels.reset_index(drop=True, inplace=True)\n",
    "annotations = human_labels.pivot(\n",
    "    index='Image ID', columns='Reader ID', values='Abnormal'\n",
    ").reset_index()\n",
    "\n",
    "m = pd.read_csv(\"cxr_model_predictions.csv\", index_col=0)\n",
    "m['img_name'] = m['img_name'].apply(lambda x: x[13:])\n",
    "m['score'] = m['score'].apply(lambda x: float(x[1:-1]))\n",
    "\n",
    "df = annotations.merge(m, left_on='Image ID', right_on='img_name')\n",
    "df = df.drop('Image ID', axis=1)\n",
    "df['model_pred_noisy'] = df['score'].apply(noise)\n",
    "df['Y_H'] = df.apply(lambda x: [x[c] for c in annotator_id_cols], axis=1)\n",
    "df['consensus'] = df['Y_H'].apply(lambda x: stats.mode(x)[0][0])\n",
    "df['model_p0'] = 1-df['score']\n",
    "df.columns = [\n",
    "    'expert1', 'expert2', 'expert3', 'expert4', 'expert5', 'img_name',\n",
    "                'model_p1', 'model_pred_noisy', 'Y_H', 'consensus','model_p0'\n",
    "]\n",
    "df = df[[\n",
    "    'expert1', 'expert2', 'expert3', 'expert4', 'expert5', 'img_name',\n",
    "                'model_p0', 'model_p1', 'model_pred_noisy', 'Y_H', 'consensus'\n",
    "]]\n",
    "df.to_csv('nih_processed.csv', index=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0299aad1-eefe-4481-b020-95f0cb7a6981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data dict for our model\n",
    "Y_H = np.array(list(df['Y_H'])) + 1\n",
    "Y_M = [[[1 - s, s]] for s in df['model_pred']]\n",
    "\n",
    "data_dict = {\n",
    "    'Y_M' : Y_M,\n",
    "    'Y_H' : Y_H.tolist(),\n",
    "    'n_models' : 1,\n",
    "    'n_humans' : n_experts,\n",
    "    'K' : 2\n",
    "}\n",
    "\n",
    "with open('data.pickle', 'wb') as handle:\n",
    "    pickle.dump(data_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec694320-391b-4a03-8c82-344d36b38899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data dict for our model with noisy predictions\n",
    "Y_M_noisy = [[[1 - s, s]] for s in df['model_pred_noisy']]\n",
    "\n",
    "data_dict_noisy = {\n",
    "    'Y_M' : Y_M_noisy,\n",
    "    'Y_H' : Y_H.tolist(),\n",
    "    'n_models' : 1,\n",
    "    'n_humans' : n_experts,\n",
    "    'K' : 2\n",
    "}\n",
    "\n",
    "with open('data_noisy.pickle', 'wb') as handle:\n",
    "    pickle.dump(data_dict_noisy, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9128490a-5eef-49d5-8c3b-1eee728061f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data dict for our model with \"distribution shift\"\n",
    "Y_M_ds = np.concatenate([\n",
    "        Y_M[:125], \n",
    "        Y_M_noisy[125:250], \n",
    "        Y_M[250:375],\n",
    "        Y_M_noisy[375:500],\n",
    "        Y_M[500:625],\n",
    "        Y_M_noisy[625:750]\n",
    "])\n",
    "\n",
    "data_dict_ds = {\n",
    "    'Y_M' : Y_M_ds,\n",
    "    'Y_H' : Y_H.tolist()[:750],\n",
    "    'n_models' : 1,\n",
    "    'n_humans' : n_experts,\n",
    "    'K' : 2\n",
    "}\n",
    "\n",
    "with open('data_ds.pickle', 'wb') as handle:\n",
    "    pickle.dump(data_dict_ds, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aeff0ad-6722-4bfb-907c-d975bbd2a693",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set (n=250):\n",
      "\tclassifier: 0.816\n",
      "\tnoisy classifier: 0.616\n",
      "\texpert 1: 0.904\n",
      "\texpert 2: 0.88\n",
      "\texpert 3: 0.848\n",
      "\texpert 4: 0.932\n",
      "\texpert 5: 0.84\n"
     ]
    }
   ],
   "source": [
    "# get accuracies\n",
    "n = 250\n",
    "print('accuracy on test set (n={}):'.format(n))\n",
    "df['model_pred_int'] = df['model_pred'].apply(lambda x: 0 if x<0.5 else 1)\n",
    "df['noisy_pred_int'] = df['model_pred_noisy'].apply(lambda x: 0 if x<0.5 else 1)\n",
    "\n",
    "test_accuracy = sum(df[:n]['model_pred_int']==df[:n]['consensus'])/n\n",
    "print(\"\\tclassifier: {}\".format(test_accuracy))\n",
    "\n",
    "test_accuracy_noisy = sum(df[:n]['noisy_pred_int']==df[:n]['consensus'])/n\n",
    "print(\"\\tnoisy classifier: {}\".format(test_accuracy_noisy))\n",
    "\n",
    "for e in range(n_experts):\n",
    "    expert_acc = sum(df[:n]['expert'+str(e+1)]==df[:n]['consensus'])/n\n",
    "    print (\"\\texpert {}: {}\".format(e+1, expert_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6541519-e265-4c93-bce8-9d2d6481a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data dict for INFEXP model\n",
    "for start_point in [0,250,500]:\n",
    "    y_h = np.array(df[['expert'+str(i+1) for i in range(n_experts)]])\n",
    "    y_h = y_h.reshape((n_experts,len(df)))\n",
    "    d_new = np.array(df['consensus'])\n",
    "\n",
    "    y_m_new = np.array([[1 - s, s] for s in df['model_pred']])\n",
    "    model_confs = np.array([y_m_new])\n",
    "    model_preds = np.array([[np.argmax(i) for i in j] for j in model_confs])\n",
    "\n",
    "    df['model_correct'] = df['model_pred_int']==df['consensus']\n",
    "    model_perf = np.array([[df['model_correct'].mean()]])\n",
    "    class_wise_perf = np.array(\n",
    "        df.groupby(\n",
    "            'consensus'\n",
    "        ).aggregate(\n",
    "            {'model_correct':'mean'}\n",
    "        )['model_correct']\n",
    "    )\n",
    "\n",
    "    n_models = 1\n",
    "    model_preds_dict_new = {\n",
    "        'model_confs' : model_confs[:,start_point:start_point+250],\n",
    "        'model_preds' : model_preds[:,start_point:start_point+250],\n",
    "        'true_targets' : d_new[start_point:start_point+250],\n",
    "        'expert_preds' : y_h[:,start_point:start_point+250],\n",
    "        'chosen_models' : np.array([0]),\n",
    "        'model_perf' : model_perf,\n",
    "        'model_perf_per_class' : class_wise_perf\n",
    "    }\n",
    "    assert \"expert_preds\" in model_preds_dict_new.keys()\n",
    "\n",
    "    with open('nih_infexp{}.pickle'.format(start_point), 'wb') as handle:\n",
    "        pickle.dump(model_preds_dict_new, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77804c8e-e1ae-489a-a54e-7e2bd13b5608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml] *",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
