{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a614db90-3657-410f-9ae5-fb421a8362cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import calibration as cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb7683f4-00ff-406f-8f1e-f5cdb3d4d1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    {\n",
    "        \"name\" : \"nih\",\n",
    "        \"n_experts\" : 5,\n",
    "        \"n_models\" : 1,\n",
    "        \"k\" : 2,\n",
    "        \"create_shuffles\": True,\n",
    "        \"dist_shift\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\" : \"chaoyang\",\n",
    "        \"n_experts\" : 3,\n",
    "        \"n_models\" : 1,\n",
    "        \"k\" : 4,\n",
    "        \"create_shuffles\": True,\n",
    "        \"dist_shift\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\" : \"cifar\",\n",
    "        \"n_experts\" : 3,\n",
    "        \"n_models\" : 1,\n",
    "        \"k\" : 3,\n",
    "        \"create_shuffles\": False,\n",
    "        \"dist_shift\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\" : \"imagenet\",\n",
    "        \"n_experts\" : 3,\n",
    "        \"n_models\" : 1,\n",
    "        \"k\" : 3,\n",
    "        \"create_shuffles\": True,\n",
    "        \"dist_shift\": True\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4253b2ea-88d3-4d80-b5b8-f2470572fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_dict(dataset_name, df_to_save, n_experts, n_models, k, ext=\"\"):\n",
    "        Y_H = np.array(df_to_save[['expert'+str(i+1) for i in range(n_experts)]]) + 1\n",
    "        Y_M = np.array(df_to_save[['model_p'+str(i) for i in range(k)]])\n",
    "        Y_M = Y_M.reshape((len(df_to_save), 1, k))\n",
    "        \n",
    "        # previously used for chaoyang--do we need this? if so let's add it to the preprocessing\n",
    "        #row_sums = Y_M.sum(axis=2)\n",
    "        #Y_M_normalized = Y_M / row_sums[:, np.newaxis]\n",
    "\n",
    "        data_dict = {\n",
    "            'Y_M' : Y_M,\n",
    "            'Y_H' : Y_H.tolist(),\n",
    "            'n_models' : n_models,\n",
    "            'n_humans' : n_experts,\n",
    "            'K' : k\n",
    "        }\n",
    "        \n",
    "        file_name = dataset_name+'/data{}{}.pickle'.format(str(shuffle_num), ext)\n",
    "        with open(file_name, 'wb') as handle:\n",
    "            pickle.dump(data_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c684e32-cc75-47bf-9031-cafe2c4bd73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_infexp_data_dict(df_to_save, n_experts, n_models, k, start_point):\n",
    "    Y_H_inf = np.array(df_to_save[['expert'+str(i+1) for i in range(n_experts)]])\n",
    "    Y_H_inf = Y_H_inf.transpose()\n",
    "    d_new = np.array(df_to_save['consensus'])\n",
    "    Y_M_inf = np.array(df_to_save[['model_p'+str(i) for i in range(k)]])\n",
    "    model_confs = np.array([Y_M_inf])\n",
    "    model_preds = np.array([[np.argmax(i) for i in j] for j in model_confs])\n",
    "\n",
    "    df_to_save['model_correct'] = df_to_save['model_pred_int']==df_to_save['consensus']\n",
    "    model_perf = np.array([[df_to_save['model_correct'].mean()]])\n",
    "    class_wise_perf = np.array(\n",
    "        df_to_save.groupby(\n",
    "            'consensus'\n",
    "        ).aggregate(\n",
    "            {'model_correct':'mean'}\n",
    "        )['model_correct']\n",
    "    )\n",
    "\n",
    "    n_tests = 250\n",
    "    infexp_dict = {\n",
    "        'model_confs' : model_confs[:,start_point:start_point+n_tests],\n",
    "        'model_preds' : model_preds[:,start_point:start_point+n_tests],\n",
    "        'targets' : d_new[start_point:start_point+n_tests],\n",
    "        'true_targets' : d_new[start_point:start_point+n_tests],\n",
    "        'expert_preds' : Y_H_inf[:,start_point:start_point+n_tests],\n",
    "        'chosen_models' : np.array([0]),\n",
    "        'model_perf' : model_perf,\n",
    "        'model_perf_per_class' : class_wise_perf\n",
    "    }\n",
    "    \n",
    "    return infexp_dict\n",
    "\n",
    "\n",
    "def create_infexp_data_dicts(\n",
    "        dataset_name, \n",
    "        df_to_save, \n",
    "        n_start_points, \n",
    "        n_experts, \n",
    "        n_models, \n",
    "        k\n",
    "    ):\n",
    "    for start_point in range(0, 250*n_start_points, 250):\n",
    "        infexp_dict = create_infexp_data_dict(\n",
    "            df_to_save, \n",
    "            n_experts, \n",
    "            n_models, \n",
    "            k, \n",
    "            start_point\n",
    "        )\n",
    "\n",
    "        file_name = dataset_name+'/infexp/infexp{}s{}.pickle'.format(\n",
    "            start_point, str(shuffle_num)\n",
    "        )\n",
    "        with open(file_name, 'wb') as handle:\n",
    "            pickle.dump(infexp_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4157db94-8a1f-4990-bd26-73f057610efc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_accuracies(df, n_experts, k):\n",
    "    n = len(df)\n",
    "    df['model_pred_int'] = df.apply(\n",
    "        lambda x: np.argmax([x['model_p'+str(i)] for i in range(k)]), axis=1\n",
    "    )\n",
    "    df['model_correct'] = df['model_pred_int']==df['consensus']\n",
    "    test_accuracy = np.mean(df['model_correct'])\n",
    "    class_wise_accs = list(df.groupby('consensus').aggregate(\n",
    "            {'model_correct':'mean'}\n",
    "    )['model_correct'])\n",
    "    print(\"classifier (overall): {}\".format(test_accuracy))\n",
    "    print(\"\\t \" + str(class_wise_accs))\n",
    "    probs = np.array(df[['model_p'+str(i) for i in range(k)]])\n",
    "    ece = cal.get_ece(probs, df['consensus'])\n",
    "    cal_error = cal.get_calibration_error(probs, df['consensus'])\n",
    "    print(\"\\tECE = {}; cal error = {}\".format(ece, cal_error))\n",
    "    \n",
    "    for e in range(n_experts):\n",
    "        e_corr_col = 'expert{}_correct'.format(e+1)\n",
    "        df[e_corr_col] = df['expert'+str(e+1)]==df['consensus']\n",
    "        expert_acc = sum(df['expert'+str(e+1)]==df['consensus'])/n\n",
    "        class_wise_accs = list(df.groupby('consensus').aggregate(\n",
    "                {e_corr_col:'mean'}\n",
    "        )[e_corr_col])\n",
    "        print (\"expert {}: {}\".format(e+1, expert_acc))\n",
    "        print(\"\\t \" + str(class_wise_accs))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "924ac4e1-62e8-4eb5-85a1-49a8993bb79a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing nih\n",
      "classifier (overall): 0.8617283950617284\n",
      "\t [0.8246268656716418, 0.8800738007380073]\n",
      "\tECE = 0.04265388838271611; cal error = 0.04647116026213043\n",
      "expert 1: 0.9049382716049382\n",
      "\t [0.9626865671641791, 0.8763837638376384]\n",
      "expert 2: 0.9259259259259259\n",
      "\t [0.917910447761194, 0.9298892988929889]\n",
      "expert 3: 0.837037037037037\n",
      "\t [0.5111940298507462, 0.9981549815498155]\n",
      "expert 4: 0.9320987654320988\n",
      "\t [0.8208955223880597, 0.9870848708487084]\n",
      "expert 5: 0.8827160493827161\n",
      "\t [0.9664179104477612, 0.8413284132841329]\n",
      "\n",
      "\n",
      "processing chaoyang\n",
      "classifier (overall): 0.8053333333333333\n",
      "\t [0.9123711340206185, 0.5529411764705883, 0.9678714859437751, 0.6715328467153284]\n",
      "\tECE = 0.12798713147640225; cal error = 0.08557761502287563\n",
      "expert 1: 0.8586666666666667\n",
      "\t [0.6752577319587629, 0.788235294117647, 0.9919678714859438, 0.9635036496350365]\n",
      "expert 2: 0.8173333333333334\n",
      "\t [0.9226804123711341, 0.6294117647058823, 0.9839357429718876, 0.5985401459854015]\n",
      "expert 3: 0.992\n",
      "\t [1.0, 0.9647058823529412, 1.0, 1.0]\n",
      "\n",
      "\n",
      "processing cifar\n",
      "classifier (overall): 0.9173504102461477\n",
      "\t [0.8249416472157386, 0.9526845637583893, 0.9601494396014943]\n",
      "\tECE = 0.06622003135145192; cal error = 0.05717442189534001\n",
      "expert 1: 0.9339603762257355\n",
      "\t [0.9976658886295432, 0.7828859060402684, 0.9985056039850561]\n",
      "expert 2: 0.8952371422853712\n",
      "\t [0.9963321107035679, 0.9949664429530202, 0.7457036114570361]\n",
      "expert 3: 0.9120472283370022\n",
      "\t [0.7152384128042681, 0.9956375838926175, 0.9970112079701121]\n",
      "\n",
      "\n",
      "processing imagenet\n",
      "classifier (overall): 0.8602878916172735\n",
      "\t [0.8108108108108109, 0.9586776859504132, 0.8235294117647058]\n",
      "\tECE = 0.03292562083490907; cal error = 0.07427006598110154\n",
      "expert 1: 0.9263336155800169\n",
      "\t [0.9662162162162162, 0.8236914600550964, 0.9786096256684492]\n",
      "expert 2: 0.9178662150719729\n",
      "\t [0.954954954954955, 0.9807162534435262, 0.8128342245989305]\n",
      "expert 3: 0.9254868755292125\n",
      "\t [0.8693693693693694, 0.9586776859504132, 0.9598930481283422]\n",
      "\n",
      "\n",
      "classifier (overall): 0.90770533446232\n",
      "\t [0.8761261261261262, 0.9724517906336089, 0.8823529411764706]\n",
      "\tECE = 0.01922308732828263; cal error = 0.053313740059053656\n",
      "expert 1: 0.9263336155800169\n",
      "\t [0.9662162162162162, 0.8236914600550964, 0.9786096256684492]\n",
      "expert 2: 0.9178662150719729\n",
      "\t [0.954954954954955, 0.9807162534435262, 0.8128342245989305]\n",
      "expert 3: 0.9254868755292125\n",
      "\t [0.8693693693693694, 0.9586776859504132, 0.9598930481283422]\n",
      "\n",
      "\n",
      "classifier (overall): 0.8484335309060118\n",
      "\t [0.8018018018018018, 0.953168044077135, 0.8021390374331551]\n",
      "\tECE = 0.03832228400501199; cal error = 0.08342974729627305\n",
      "expert 1: 0.9263336155800169\n",
      "\t [0.9662162162162162, 0.8236914600550964, 0.9786096256684492]\n",
      "expert 2: 0.9178662150719729\n",
      "\t [0.954954954954955, 0.9807162534435262, 0.8128342245989305]\n",
      "expert 3: 0.9254868755292125\n",
      "\t [0.8693693693693694, 0.9586776859504132, 0.9598930481283422]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dat in datasets:\n",
    "    dataset_name = dat['name']\n",
    "    print('processing '+ dataset_name)\n",
    "    df = pd.read_csv(dataset_name+'/data_clean.csv')\n",
    "    \n",
    "    n_experts, n_models, k = dat['n_experts'], dat['n_models'], dat['k']\n",
    "    get_accuracies(df, n_experts, k)\n",
    "    \n",
    "    if dat[\"create_shuffles\"]:\n",
    "\n",
    "        for shuffle_num in [\"\", 1,2,3]:\n",
    "            if shuffle_num == \"\":\n",
    "                df_to_save = df\n",
    "            else:\n",
    "                df_to_save = df.sample(frac=1, random_state=shuffle_num)\n",
    "\n",
    "            # create & save data dict for our method\n",
    "            create_data_dict(dataset_name, df_to_save, n_experts, n_models, k)\n",
    "\n",
    "            # create & save data dicts for infexp method\n",
    "            n_start_points=3\n",
    "            create_infexp_data_dicts(\n",
    "                dataset_name, \n",
    "                df_to_save, \n",
    "                n_start_points, \n",
    "                n_experts, \n",
    "                n_models, \n",
    "                k\n",
    "            )\n",
    "                    \n",
    "    else:\n",
    "        create_data_dict(dataset_name, df, n_experts, n_models, k)\n",
    "            \n",
    "        n_start_points=12\n",
    "        create_infexp_data_dicts(\n",
    "            dataset_name, \n",
    "            df_to_save, \n",
    "            n_start_points, \n",
    "            n_experts, \n",
    "            n_models, \n",
    "            k\n",
    "        )\n",
    "        \n",
    "    if dat[\"dist_shift\"]:\n",
    "        assert dat[\"create_shuffles\"] # otherwise not implemented\n",
    "        dat_before_ds = pd.read_csv(dataset_name+\"/data_before_ds_clean.csv\")\n",
    "        get_accuracies(dat_before_ds, n_experts, k)\n",
    "        dat_after_ds = pd.read_csv(dataset_name+\"/data_after_ds_clean.csv\")\n",
    "        get_accuracies(dat_after_ds, n_experts, k)\n",
    "        \n",
    "        for shuffle_num in [\"\", 1,2,3]:\n",
    "            if shuffle_num == \"\":\n",
    "                dat_before_to_save = dat_before_ds\n",
    "                dat_after_to_save = dat_after_ds\n",
    "            else:\n",
    "                dat_before_to_save = dat_before_ds.sample(\n",
    "                    frac=1, random_state=shuffle_num\n",
    "                )\n",
    "                dat_after_to_save = dat_after_ds.sample(\n",
    "                    frac=1, random_state=shuffle_num\n",
    "                )\n",
    "\n",
    "            df_to_save = pd.concat([\n",
    "                dat_before_to_save[:125], \n",
    "                dat_after_to_save[:125], \n",
    "                dat_before_to_save[125:250], \n",
    "                dat_after_to_save[125:250],\n",
    "                dat_before_to_save[250:],\n",
    "                dat_after_to_save[250:]\n",
    "            ])\n",
    "\n",
    "            create_data_dict(\n",
    "                dataset_name, \n",
    "                df_to_save, \n",
    "                n_experts, \n",
    "                n_models, \n",
    "                k, \n",
    "                ext=\"_ds\".format(str(shuffle_num))\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml] *",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
