{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "270f8028-c8e4-49fc-8882-3c53b6f01f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy import stats\n",
    "import calibration as cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98849847-c1d4-4397-b903-7e7b17f45f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minority_opinion(row, cols):\n",
    "    votes = [v for v in row[cols].tolist() if pd.notnull(v)]\n",
    "    modes = stats.mode(votes, keepdims=True)[0]\n",
    "    for val in votes:\n",
    "        if val not in modes:\n",
    "            return val\n",
    "    return np.random.choice(list(votes))\n",
    "\n",
    "def add_minority(df, use_cols, start, end, noise_levels=[110,125]):\n",
    "    col_name = '_minority{}-{}'.format(start, end)\n",
    "    include_vals = [i for i in use_cols if int(i[0])<end and int(i[0])>=start]\n",
    "    include_vals = [i for i in include_vals if int(i[1]) in noise_levels]\n",
    "    df[col_name] = df.apply(\n",
    "        lambda x: get_minority_opinion(x, include_vals), axis=1\n",
    "    )\n",
    "\n",
    "def add_consensus(df, use_cols, start, end,noise_levels=[110,125]):\n",
    "    def find_consensus(row, cols):\n",
    "        votes = [v for v in row[cols].tolist() if pd.notnull(v)]\n",
    "        return stats.mode(votes, keepdims=True)[0][0]\n",
    "    col_name = '_consensus{}-{}'.format(start, end)\n",
    "    include_vals = [i for i in use_cols if int(i[0])<end and int(i[0])>start]\n",
    "    include_vals = [i for i in include_vals if int(i[1]) in noise_levels]\n",
    "    df[col_name] = df.apply(lambda x: find_consensus(x, include_vals), axis=1)\n",
    "    \n",
    "def create_experts(row, exp1_c, exp2_c, exp3_c, s1, e1, s2, e2, s3, e3):\n",
    "    if row['_consensus'] in exp1_c:\n",
    "        row['expert1'] = row['_consensus{}-{}'.format(s1, e1)]\n",
    "    else:\n",
    "        row['expert1'] = row['_minority{}-{}'.format(s1, e1)]\n",
    "    if row['_consensus'] in exp2_c:\n",
    "        row['expert2'] = row['_consensus{}-{}'.format(s2, e2)]\n",
    "    else:\n",
    "        row['expert2'] = row['_minority{}-{}'.format(s2, e2)] \n",
    "    if row['_consensus'] in exp3_c:\n",
    "        row['expert3'] = row['_consensus{}-{}'.format(s3, e3)]\n",
    "    else:\n",
    "        row['expert3'] = row['_minority{}-{}'.format(s3, e3)]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51de3f2a-2f13-4e65-952d-78f74c1f2705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_consensus(y):\n",
    "    most = max(list(map(y.count, y)))\n",
    "    modes = list(set(filter(lambda x: y.count(x) == most, y)))\n",
    "    if len(modes) > 1:\n",
    "        return None\n",
    "    return modes[0]\n",
    "\n",
    "def get_consensus_df(row):\n",
    "    y = row.tolist()\n",
    "    most = max(list(map(y.count, y)))\n",
    "    modes = list(set(filter(lambda x: y.count(x) == most, y)))\n",
    "    if len(modes) > 1:\n",
    "        return None\n",
    "    return modes[0]\n",
    "\n",
    "def process_expert_votes(row):\n",
    "    votes = [v for v in row.tolist() if pd.notnull(v)]\n",
    "    row['_consensus'] = get_consensus(votes)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a06510e-dbee-4874-94c3-d01cf0f702d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "df_human_proc = pd.DataFrame(\n",
    "    df_human.groupby(['image_name', 'noise_level']).apply(combine_experts),\n",
    "    columns=[\"Y_H\"]\n",
    ")\n",
    "df_human_proc.reset_index(inplace=True)\n",
    "df_human_proc['consensus'] = df_human_proc['Y_H'].apply(\n",
    "    lambda x: stats.mode(x, keepdims=True)[0][0] if stats.mode(x, keepdims=True)[1][0] > 1 else -1\n",
    ")\n",
    "for e in range(n_experts):\n",
    "    df_human_proc['expert'+str(e+1)] = df_human_proc['Y_H'].apply(lambda x: x[e])\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "611a743e-76a8-4a07-aa1f-c0807511a08c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def combine_experts(rows):\n",
    "    true_result = rows.iloc[0]['image_category_new']\n",
    "    expert_predictions = rows['participant_classification_new']\n",
    "    \n",
    "    correct_responses = []\n",
    "    incorrect_responses = []\n",
    "    for pred in expert_predictions:\n",
    "        if pred==true_result:\n",
    "            correct_responses.append(pred)\n",
    "        else:\n",
    "            incorrect_responses.append(pred)\n",
    "            \n",
    "    expert_predictions = []\n",
    "    if len(correct_responses)>=2:\n",
    "        expert_predictions = correct_responses[:2]\n",
    "        if len(incorrect_responses)>=1:\n",
    "            expert_predictions.append(incorrect_responses[0])\n",
    "        else:\n",
    "            expert_predictions.append(correct_responses[2])\n",
    "    elif len(correct_responses)==1:\n",
    "        expert_predictions = [correct_responses[0]]\n",
    "        expert_predictions.extend(incorrect_responses[:2])\n",
    "    else:\n",
    "        expert_predictions = incorrect_responses[:3]\n",
    "    if true_result == 1:\n",
    "        expert_predictions = [expert_predictions[2], expert_predictions[0], expert_predictions[1]]\n",
    "    elif true_result == 2:\n",
    "        expert_predictions = [expert_predictions[1], expert_predictions[2], expert_predictions[0]]\n",
    "        \n",
    "    return expert_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7810f3f7-1199-4499-9129-f90c26089b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class1 = ['clock', 'knife', 'oven', 'chair', 'bottle', 'keyboard']\n",
    "class2 = ['cat', 'elephant', 'dog', 'bird', 'bear']\n",
    "class3 = ['airplane', 'boat', 'car', 'truck', 'bicycle']\n",
    "\n",
    "def convert_to_tri_class(x, c1, c2, c3):\n",
    "    if x in c1:\n",
    "        c = 0\n",
    "    elif x in c2:\n",
    "        c = 1\n",
    "    else:\n",
    "        assert x in c3\n",
    "        c = 2\n",
    "    return c\n",
    "\n",
    "def convert_prob_to_tri_class(row, c1, c2, c3):\n",
    "    c1_sum = 0\n",
    "    for v in c1:\n",
    "        c1_sum += row[v]\n",
    "    c2_sum = 0\n",
    "    for v in c2:\n",
    "        c2_sum += row[v]\n",
    "    c3_sum = 0\n",
    "    for v in c3:\n",
    "        c3_sum += row[v]\n",
    "    row['model_p0'] = min(1.0, c1_sum)\n",
    "    row['model_p1'] = min(1.0, c2_sum)\n",
    "    row['model_p2'] = min(1.0, c3_sum)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "117faa6d-8be6-4190-bf75-749986bbe712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of expert and model predictions\n",
    "n_experts = 3\n",
    "df_model = pd.read_csv(\"model_preds_raw.csv\")\n",
    "df_human = pd.read_csv(\"annotations_raw.csv\")\n",
    "\n",
    "df_model = df_model[df_model['noise_level']==120]\n",
    "\n",
    "df_human = df_human[[\n",
    " 'participant_id', 'image_id', 'image_name', 'noise_level', 'image_category',\n",
    " 'participant_classification', 'confidence', 'correct', 'total_accuracy'\n",
    "]]\n",
    "for c in ['participant_classification', 'image_category']:\n",
    "    df_human[c+\"_new\"] = df_human[c].apply(\n",
    "        convert_to_tri_class, args=(class1, class2, class3,)\n",
    "    )\n",
    "    \n",
    "df_model = df_model.apply(\n",
    "    convert_prob_to_tri_class, args=(class1, class2, class3,), axis=1\n",
    ")\n",
    "\n",
    "model_name='alexnet'\n",
    "dn_df = df_model[df_model['model_name']==model_name].copy()\n",
    "dn_df = dn_df[['image_name', 'noise_level','model_p0','model_p1', 'model_p2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e1f0f08-f35e-46d5-b1bf-1549e8df40c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_human = df_human.drop_duplicates(subset=['participant_id','noise_level','image_name'])\n",
    "df_human_p = df_human.pivot(\n",
    "    columns=['participant_id','noise_level'],\n",
    "    index='image_name',\n",
    "    values='participant_classification_new'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8801f198-adfc-4f6e-97ca-0958f64b0fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cols = df_human_p.columns\n",
    "df_human_f = df_human_p.apply(process_expert_votes, axis=1)\n",
    "\n",
    "start1 = 0; end1 = 67\n",
    "start2 = 67; end2 = 133\n",
    "start3 = 133; end3 = 200\n",
    "\n",
    "add_consensus(df_human_f,use_cols,start1,end1,noise_levels=[95,110])\n",
    "add_minority(df_human_f,use_cols,start1,end1,noise_levels=[95,110])\n",
    "add_consensus(df_human_f,use_cols,start2,end2,noise_levels=[110,125])\n",
    "add_minority(df_human_f,use_cols,start2,end2,noise_levels=[110,125])\n",
    "add_consensus(df_human_f,use_cols,start3,end3,noise_levels=[95,110])\n",
    "add_minority(df_human_f,use_cols,start3,end3,noise_levels=[95,110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35e886a1-e508-4fa3-a82a-42979aa00c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_human_f.columns = [\n",
    "    str(i[0])+'-'+str(i[1]) if i[1]!='' else str(i[0]) \n",
    "        for i in df_human_f.columns\n",
    "]\n",
    "df_human_f = df_human_f[pd.notnull(df_human_f[\"_consensus\"])]\n",
    "\n",
    "c1=[0]; c2=[1]; c3=[2]\n",
    "df_human_f = df_human_f.apply(lambda x: create_experts(\n",
    "    x, c1 + c3, c1 + c2, c2 + c3,\n",
    "    start1, end1, start2, end2, start3, end3\n",
    "), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d625b98e-c6c5-487f-ab82-2cb3ba560399",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_human_f['consensus'] = df_human_f[['expert1','expert2','expert3']].apply(\n",
    "    get_consensus_df, axis=1\n",
    ")\n",
    "df_human_f.reset_index(inplace=True)\n",
    "df_human_f = df_human_f[pd.notnull(df_human_f[\"consensus\"])]\n",
    "df_human_f['consensus'] = df_human_f['consensus'].astype(int)\n",
    "cols = ['expert1','expert2','expert3']\n",
    "for col in cols:\n",
    "    df_human_f[col] = df_human_f[col].astype(int)\n",
    "df_human_final = df_human_f[['image_name','expert1','expert2','expert3','consensus']]\n",
    "df = df_human_final.merge(dn_df, on=['image_name'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "999fbe66-7ae0-41f5-9410-2e14f2409b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>expert1</th>\n",
       "      <th>expert2</th>\n",
       "      <th>expert3</th>\n",
       "      <th>consensus</th>\n",
       "      <th>noise_level</th>\n",
       "      <th>model_p0</th>\n",
       "      <th>model_p1</th>\n",
       "      <th>model_p2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n04099969_6944</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>0.794810</td>\n",
       "      <td>0.165056</td>\n",
       "      <td>0.040133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n02504013_1455</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>0.012092</td>\n",
       "      <td>0.984549</td>\n",
       "      <td>0.003358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n04111531_10555</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>0.916009</td>\n",
       "      <td>0.003535</td>\n",
       "      <td>0.080456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n02504013_6118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>0.007704</td>\n",
       "      <td>0.990873</td>\n",
       "      <td>0.001423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n02132136_7584</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>0.016555</td>\n",
       "      <td>0.978589</td>\n",
       "      <td>0.004857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        image_name  expert1  expert2  expert3  consensus  noise_level  \\\n",
       "0   n04099969_6944        0        0        0          0          120   \n",
       "1   n02504013_1455        1        1        2          1          120   \n",
       "2  n04111531_10555        0        2        0          0          120   \n",
       "3   n02504013_6118        1        1        1          1          120   \n",
       "4   n02132136_7584        1        1        1          1          120   \n",
       "\n",
       "   model_p0  model_p1  model_p2  \n",
       "0  0.794810  0.165056  0.040133  \n",
       "1  0.012092  0.984549  0.003358  \n",
       "2  0.916009  0.003535  0.080456  \n",
       "3  0.007704  0.990873  0.001423  \n",
       "4  0.016555  0.978589  0.004857  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle rows\n",
    "np.random.seed(1)\n",
    "order = np.array([i for i in range(len(df))])\n",
    "np.random.shuffle(order)\n",
    "df = df.loc[order]\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "df.to_csv('data_clean.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1012e3ca-bdb4-4255-be94-715d1adaf176",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/markellekelly/miniconda3/envs/ml/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/markellekelly/miniconda3/envs/ml/lib/python3.8/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expert1\n",
      "expert2\n",
      "expert3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'a' cannot be empty unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m add_minority(dat,use_cols,start1,end1,noise_levels\u001b[38;5;241m=\u001b[39m[noise_level])\n\u001b[1;32m     23\u001b[0m add_consensus(dat,use_cols,start2,end2,noise_levels\u001b[38;5;241m=\u001b[39m[noise_level])\n\u001b[0;32m---> 24\u001b[0m \u001b[43madd_minority\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdat\u001b[49m\u001b[43m,\u001b[49m\u001b[43muse_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstart2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mend2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnoise_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnoise_level\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m add_consensus(dat,use_cols,start3,end3,noise_levels\u001b[38;5;241m=\u001b[39m[noise_level])\n\u001b[1;32m     26\u001b[0m add_minority(dat,use_cols,start3,end3,noise_levels\u001b[38;5;241m=\u001b[39m[noise_level])\n",
      "Cell \u001b[0;32mIn [2], line 13\u001b[0m, in \u001b[0;36madd_minority\u001b[0;34m(df, use_cols, start, end, noise_levels)\u001b[0m\n\u001b[1;32m     11\u001b[0m include_vals \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m use_cols \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(i[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m<\u001b[39mend \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mint\u001b[39m(i[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39mstart]\n\u001b[1;32m     12\u001b[0m include_vals \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m include_vals \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(i[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;129;01min\u001b[39;00m noise_levels]\n\u001b[0;32m---> 13\u001b[0m df[col_name] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_minority_opinion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_vals\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/pandas/core/frame.py:7547\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   7536\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m   7538\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m   7539\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   7540\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   7545\u001b[0m     kwds\u001b[38;5;241m=\u001b[39mkwds,\n\u001b[1;32m   7546\u001b[0m )\n\u001b[0;32m-> 7547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/pandas/core/apply.py:180\u001b[0m, in \u001b[0;36mFrameApply.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/pandas/core/apply.py:255\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 255\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/pandas/core/apply.py:284\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    286\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    287\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    288\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn [2], line 14\u001b[0m, in \u001b[0;36madd_minority.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     11\u001b[0m include_vals \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m use_cols \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(i[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m<\u001b[39mend \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mint\u001b[39m(i[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39mstart]\n\u001b[1;32m     12\u001b[0m include_vals \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m include_vals \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(i[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;129;01min\u001b[39;00m noise_levels]\n\u001b[1;32m     13\u001b[0m df[col_name] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mget_minority_opinion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_vals\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     15\u001b[0m )\n",
      "Cell \u001b[0;32mIn [2], line 7\u001b[0m, in \u001b[0;36mget_minority_opinion\u001b[0;34m(row, cols)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m modes:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m val\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvotes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mmtrand.pyx:915\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'a' cannot be empty unless no samples are taken"
     ]
    }
   ],
   "source": [
    "# create distribution-shift versions\n",
    "df_model = pd.read_csv(\"model_preds_raw.csv\")\n",
    "df_model = df_model.apply(\n",
    "    convert_prob_to_tri_class, args=(class1, class2, class3,), axis=1\n",
    ")\n",
    "model_name='alexnet'\n",
    "dn_df = df_model[df_model['model_name']==model_name].copy()\n",
    "dn_df = dn_df[['image_name', 'noise_level','model_p0','model_p1', 'model_p2']]\n",
    "dn_df_before_ds = dn_df[dn_df['noise_level']==80].reset_index(drop=True)\n",
    "dn_df_after_ds = dn_df[dn_df['noise_level']==125].reset_index(drop=True)\n",
    "\n",
    "start1 = 0; end1 = 67\n",
    "start2 = 67; end2 = 133\n",
    "start3 = 133; end3 = 200\n",
    "\n",
    "df_human_base = df_human_p.apply(process_expert_votes, axis=1)\n",
    "df_human_f_before = df_human_base.copy()\n",
    "df_human_f_after = df_human_base.copy()\n",
    "\n",
    "for dat, noise_level in zip([df_human_f_before, df_human_f_after],[80, 125]):\n",
    "    add_consensus(dat,use_cols,start1,end1,noise_levels=[noise_level])\n",
    "    add_minority(dat,use_cols,start1,end1,noise_levels=[noise_level])\n",
    "    add_consensus(dat,use_cols,start2,end2,noise_levels=[noise_level])\n",
    "    add_minority(dat,use_cols,start2,end2,noise_levels=[noise_level])\n",
    "    add_consensus(dat,use_cols,start3,end3,noise_levels=[noise_level])\n",
    "    add_minority(dat,use_cols,start3,end3,noise_levels=[noise_level])\n",
    "    \n",
    "    dat.columns = [\n",
    "        str(i[0])+'-'+str(i[1]) if i[1]!='' else str(i[0]) \n",
    "            for i in dat.columns\n",
    "    ]\n",
    "\n",
    "    dat = dat[pd.notnull(dat[\"_consensus\"])]\n",
    "\n",
    "    c1=[0]; c2=[1]; c3=[2]\n",
    "    dat = dat.apply(lambda x: create_experts(\n",
    "        x, c1 + c3, c1 + c2, c2 + c3,\n",
    "        start1, end1, start2, end2, start3, end3\n",
    "    ), axis=1)\n",
    "\n",
    "    dat['consensus'] = dat[['expert1','expert2','expert3']].apply(\n",
    "        get_consensus_df, axis=1\n",
    "    )\n",
    "    dat.reset_index(inplace=True)\n",
    "    dat = dat[pd.notnull(dat[\"consensus\"])]\n",
    "    dat['consensus'] = dat['consensus'].astype(int)\n",
    "    cols = ['expert1','expert2','expert3']\n",
    "    for col in cols:\n",
    "        print(col)\n",
    "        dat = dat[pd.notnull(dat[col])]\n",
    "        dat[col] = dat[col].astype(int)\n",
    "    dat = dat[['image_name','expert1','expert2','expert3','consensus']]\n",
    "\n",
    "df_before_ds = df_human_f_before.merge(dn_df_before_ds, on=['image_name'], how='inner')\n",
    "df_after_ds = df_human_f_after.merge(dn_df_after_ds, on=['image_name'], how='inner')\n",
    "\n",
    "df_before_ds.to_csv('data_before_ds_clean.csv')\n",
    "df_after_ds.to_csv('data_after_ds_clean.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55d1c14d-5da4-4553-b68e-75392e4386c1",
   "metadata": {},
   "source": [
    "# save data dict for our model\n",
    "y_h = np.array(df[['expert1','expert2','expert3']]) + 1\n",
    "y_m = np.array(df[['model_p'+str(i) for i in range(3)]]).reshape((len(df), 1, 3))\n",
    "\n",
    "out_dict = {\n",
    "    'Y_H' : y_h.tolist(),\n",
    "    'Y_M' : y_m.tolist(),\n",
    "    'n_models': 1,\n",
    "    'n_humans': n_experts,\n",
    "    'K': 3\n",
    "}\n",
    "\n",
    "ext = \"\"\n",
    "if noisy:\n",
    "    ext += \"_noisy\"\n",
    "if distribution_shift:\n",
    "    ext += \"_ds\"+str(ds_num)\n",
    "if True:\n",
    "    ext += \"_exp\"\n",
    "    \n",
    "with open('data' + ext + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(out_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58be2323-ccd9-46c0-9dff-376cff53e152",
   "metadata": {},
   "source": [
    "y_h = np.array(df[['expert1','expert2','expert3']]) + 1\n",
    "y_m = np.array(df[['model_p'+str(i) for i in range(3)]]).reshape((len(df), 1, 3))\n",
    "\n",
    "for shuffle_num in [1,2,3]:\n",
    "    df_shuffled = df.sample(frac=1, random_state=shuffle_num)\n",
    "\n",
    "    y_h = np.array(df_shuffled[['expert1','expert2','expert3']]) + 1\n",
    "    y_m = np.array(df_shuffled[['model_p'+str(i) for i in range(3)]]).reshape((len(df_shuffled), 1, 3))\n",
    "\n",
    "\n",
    "    out_dict = {\n",
    "        'Y_H' : y_h.tolist(),\n",
    "        'Y_M' : y_m.tolist(),\n",
    "        'n_models': 1,\n",
    "        'n_humans': n_experts,\n",
    "        'K': 3\n",
    "    }\n",
    "\n",
    "\n",
    "    with open('data{}.pickle'.format(shuffle_num), 'wb') as handle:\n",
    "        pickle.dump(out_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b27b98-daf9-436c-b13d-918cb50a4bc3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create data dict for INFEXP model\n",
    "for start_point in [0, 250, 500]:\n",
    "    y_h = np.array(df[['expert'+str(i+1) for i in range(n_experts)]])\n",
    "    y_h = y_h.transpose()\n",
    "    d_new = np.array(df['consensus'])\n",
    "\n",
    "    y_m_new = np.array(df[['model_p'+str(i) for i in range(3)]])\n",
    "    model_confs = np.array([y_m_new])\n",
    "    model_preds = np.array([[np.argmax(i) for i in j] for j in model_confs])\n",
    "\n",
    "    df['model_correct'] = df['model_pred_int']==df['consensus']\n",
    "    model_perf = np.array([[df['model_correct'].mean()]])\n",
    "    class_wise_perf = np.array(\n",
    "        df.groupby(\n",
    "            'consensus'\n",
    "        ).aggregate(\n",
    "            {'model_correct':'mean'}\n",
    "        )['model_correct']\n",
    "    )\n",
    "\n",
    "    n_models = 1\n",
    "    n_tests = 250\n",
    "    infexp_dict = {\n",
    "        'model_confs' : model_confs[:,start_point:start_point+n_tests],\n",
    "        'model_preds' : model_preds[:,start_point:start_point+n_tests],\n",
    "        'targets' : d_new[start_point:start_point+n_tests],\n",
    "        'true_targets' : d_new[start_point:start_point+n_tests],\n",
    "        'expert_preds' : y_h[:,start_point:start_point+n_tests],\n",
    "        'chosen_models' : np.array([0]),\n",
    "        'model_perf' : model_perf,\n",
    "        'model_perf_per_class' : class_wise_perf\n",
    "    }\n",
    "\n",
    "    with open('imagenet_infexp{}_exp.pickle'.format(start_point), 'wb') as handle:\n",
    "        pickle.dump(infexp_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "663deb69-7103-4118-aa6a-201f596093de",
   "metadata": {},
   "source": [
    "# create data dict for INFEXP model\n",
    "for shuffle_num in [\"\", 1, 2, 3]:\n",
    "    for start_point in [0, 250, 500]:\n",
    "        if shuffle_num != \"\":\n",
    "            df_shuffled = df.sample(frac=1, random_state=shuffle_num)\n",
    "        else:\n",
    "            df_shuffled = df.copy()\n",
    "        y_h = np.array(df_shuffled[['expert'+str(i+1) for i in range(n_experts)]])\n",
    "        y_h = y_h.transpose()\n",
    "        d_new = np.array(df_shuffled['consensus'])\n",
    "\n",
    "        y_m_new = np.array(df_shuffled[['model_p'+str(i) for i in range(3)]])\n",
    "        model_confs = np.array([y_m_new])\n",
    "        model_preds = np.array([[np.argmax(i) for i in j] for j in model_confs])\n",
    "\n",
    "        df_shuffled['model_correct'] = df_shuffled['model_pred_int']==df_shuffled['consensus']\n",
    "        model_perf = np.array([[df_shuffled['model_correct'].mean()]])\n",
    "        class_wise_perf = np.array(\n",
    "            df_shuffled.groupby(\n",
    "                'consensus'\n",
    "            ).aggregate(\n",
    "                {'model_correct':'mean'}\n",
    "            )['model_correct']\n",
    "        )\n",
    "\n",
    "        n_models = 1\n",
    "        n_tests = 250\n",
    "        infexp_dict = {\n",
    "            'model_confs' : model_confs[:,start_point:start_point+n_tests],\n",
    "            'model_preds' : model_preds[:,start_point:start_point+n_tests],\n",
    "            'targets' : d_new[start_point:start_point+n_tests],\n",
    "            'true_targets' : d_new[start_point:start_point+n_tests],\n",
    "            'expert_preds' : y_h[:,start_point:start_point+n_tests],\n",
    "            'chosen_models' : np.array([0]),\n",
    "            'model_perf' : model_perf,\n",
    "            'model_perf_per_class' : class_wise_perf\n",
    "        }\n",
    "\n",
    "        with open('imagenet_infexp{}s{}.pickle'.format(start_point, str(shuffle_num)), 'wb') as handle:\n",
    "            pickle.dump(infexp_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e95d85e-b323-45f5-801f-8a6ce102aad9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
