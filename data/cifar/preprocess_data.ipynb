{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c174c60-d02b-4fd5-91e6-105425889e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import calibration as cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "802ec417-ca3d-4b9c-b01e-6d076a2d86a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcs for combining expert opinions\n",
    "def get_minority_opinion(x):\n",
    "    modes = stats.mode(x, keepdims=True)[0]\n",
    "    for val in x:\n",
    "        if val not in modes:\n",
    "            return val\n",
    "    return np.random.choice(list(x))\n",
    "\n",
    "def add_minority(df, start, end):\n",
    "    col_name = 'minority{}-{}'.format(start, end)\n",
    "    include_vals = [i for i in range(start, end+1)]\n",
    "    df[col_name] = df.apply(\n",
    "        lambda x: get_minority_opinion(x[include_vals]), axis=1\n",
    "    )\n",
    "\n",
    "def add_consensus(df, start, end):\n",
    "    col_name = 'consensus{}-{}'.format(start, end)\n",
    "    include_vals = [i for i in range(start, end+1)]\n",
    "    df[col_name] = df.apply(lambda x: stats.mode(\n",
    "        x[include_vals], keepdims=True\n",
    "    )[0][0], axis=1)\n",
    "    \n",
    "def create_experts(row, exp1_c, exp2_c, exp3_c):\n",
    "    if row['consensus'] in exp1_c:\n",
    "        row['expert1'] = row['consensus0-10']\n",
    "    else:\n",
    "        row['expert1'] = row['minority0-15']\n",
    "    if row['consensus'] in exp2_c:\n",
    "        row['expert2'] = row['consensus16-26']\n",
    "    else:\n",
    "        row['expert2'] = row['minority16-32'] \n",
    "    if row['consensus'] in exp3_c:\n",
    "        row['expert3'] = row['consensus33-43']\n",
    "    else:\n",
    "        row['expert3'] = row['minority33-48']\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88b04d5f-bb49-4d69-969b-7766ed5373ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcs for merging classes into super-classes\n",
    "def combine_classes(row, class1, class2, class3):\n",
    "    for col in ['expert1', 'expert2', 'expert3','consensus']:\n",
    "        if row[col] in class1:\n",
    "            row[col] = 0\n",
    "        elif row[col] in class2:\n",
    "            row[col] = 1\n",
    "        else:\n",
    "            row[col] = 2\n",
    "    return row\n",
    "\n",
    "def combine_model_classes(row, c1, c2, c3):\n",
    "    m1 = row[0]\n",
    "    c1_prob = min(1.0, sum(m1[c1]))\n",
    "    c2_prob = min(1.0, sum(m1[c2]))\n",
    "    c3_prob = min(1.0, sum(m1[c3]))\n",
    "    return [[c1_prob, c2_prob, c3_prob]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdf2293b-b1fd-4448-886e-aa05f2627881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of expert and model predictions\n",
    "# new classes (e.g., class c1 is original classes 0, 1, and 2)\n",
    "c1 = [0,1,2]\n",
    "c2 = [3,4,5]\n",
    "c3 = [6,7,8,9]\n",
    "n_experts = 3\n",
    "\n",
    "expert_preds = pd.read_pickle(r'cifar_raw_annotations.pkl')\n",
    "df = pd.DataFrame(expert_preds)\n",
    "df['consensus'] = df.apply(lambda x: stats.mode(x, keepdims=True)[0][0], axis=1)\n",
    "\n",
    "d = pd.read_pickle(r'cifar_raw_model_data.pkl')\n",
    "chosen_model = 55 # used 28, 46, 55\n",
    "n_models = 1\n",
    "chosen_model_preds = d['model_confs'][chosen_model]\n",
    "y_m = chosen_model_preds.reshape((10000,n_models,10))\n",
    "y_m_new = np.array([combine_model_classes(x, c1, c2, c3) for x in y_m])\n",
    "for i in range(3):\n",
    "    df['model_p'+str(i)] = y_m_new[:,0,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9a285aa-56f2-4fd0-8709-cd155e2e4509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create experts\n",
    "\n",
    "# expert 1 (annotations 0-15)\n",
    "add_consensus(df, 0, 10)\n",
    "add_minority(df, 0, 15)\n",
    "# expert 2 (annotations 16-32)\n",
    "add_consensus(df, 16, 26)\n",
    "add_minority(df, 16, 32)\n",
    "# expert 3 (annotations 33-48)\n",
    "add_consensus(df, 33, 43)\n",
    "add_minority(df, 33, 48)\n",
    "\n",
    "df = df.apply(lambda x: create_experts(x, c1 + c3, c1 + c2, c2 + c3), axis=1)\n",
    "df = df.apply(lambda x: combine_classes(x, c1, c2, c3), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64b2651b-58a9-4161-b058-8340f4174a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "df = df[['expert1', 'expert2', 'expert3', 'consensus', 'model_p0', 'model_p1', 'model_p2']].copy()\n",
    "\n",
    "for c in ['expert1', 'expert2', 'expert3', 'consensus']:\n",
    "    df[c] = df[c].astype(int)\n",
    "\n",
    "def has_true_consensus(row):\n",
    "    if row['expert1']!=row['expert2'] and row['expert2']!=row['expert3'] and row['expert1']!=row['expert3']:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "cons = df.apply(has_true_consensus, axis=1)\n",
    "df = df[cons]\n",
    "df.to_csv('data_clean.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7bfebe6-3a25-440d-bce2-6f2abd473778",
   "metadata": {},
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95df55be-154e-4d78-a7e6-0f4f6ec30882",
   "metadata": {
    "tags": []
   },
   "source": [
    "# get accuracies\n",
    "n = 3000\n",
    "print('accuracy on test set (n={}):'.format(n))\n",
    "\n",
    "df['model_correct'] = df['model_pred_int']==df['consensus']\n",
    "test_accuracy = np.mean(df[:n]['model_correct'])\n",
    "class_wise_accs = list(df[:n].groupby('consensus').aggregate(\n",
    "        {'model_correct':'mean'}\n",
    ")['model_correct'])\n",
    "print(\"\\tclassifier (overall): {}\".format(test_accuracy))\n",
    "print(\"\\t\\t \" + str(class_wise_accs))\n",
    "probs = np.array(df[['model_p0','model_p1','model_p2']])\n",
    "ece = cal.get_ece(probs, df['consensus'])\n",
    "cal_error = cal.get_calibration_error(probs, df['consensus'])\n",
    "print(\"\\t\\tECE = {}; cal error = {}\".format(ece, cal_error))\n",
    "\n",
    "\n",
    "for e in range(n_experts):\n",
    "    e_corr_col = 'expert{}_correct'.format(e+1)\n",
    "    df[e_corr_col] = df['expert'+str(e+1)]==df['consensus']\n",
    "    expert_acc = sum(df[:n]['expert'+str(e+1)]==df[:n]['consensus'])/n\n",
    "    class_wise_accs = list(df[:n].groupby('consensus').aggregate(\n",
    "            {e_corr_col:'mean'}\n",
    "    )[e_corr_col])\n",
    "    print (\"\\texpert {}: {}\".format(e+1, expert_acc))\n",
    "    print(\"\\t\\t \" + str(class_wise_accs))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3046c19f-1a2d-41bc-8715-511857cf2df1",
   "metadata": {},
   "source": [
    "# save data dict for our model\n",
    "y_h = np.array(df[['expert1','expert2','expert3']]) + 1\n",
    "y_m = np.array(df[['model_p'+str(i) for i in range(3)]]).reshape((len(df), 1, 3))\n",
    "\n",
    "out_dict = {\n",
    "    'Y_H' : y_h.tolist(),\n",
    "    'Y_M' : y_m.tolist(),\n",
    "    'n_models': n_models,\n",
    "    'n_humans': n_experts,\n",
    "    'K': 3\n",
    "}\n",
    "\n",
    "with open('data.pickle', 'wb') as handle:\n",
    "    pickle.dump(out_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0539ccb0-d7e0-4384-afa2-3ddeee14696b",
   "metadata": {},
   "source": [
    "# create data dict for INFEXP model\n",
    "for start_point in range(0, 3000, 250):\n",
    "    y_h = np.array(df[['expert'+str(i+1) for i in range(n_experts)]])\n",
    "    y_h = y_h.transpose()\n",
    "    d_new = np.array(df['consensus'])\n",
    "\n",
    "    y_m_new = np.array(df[['model_p'+str(i) for i in range(3)]])\n",
    "    model_confs = np.array([y_m_new])\n",
    "    model_preds = np.array([[np.argmax(i) for i in j] for j in model_confs])\n",
    "\n",
    "    df['model_correct'] = df['model_pred_int']==df['consensus']\n",
    "    model_perf = np.array([[df['model_correct'].mean()]])\n",
    "    class_wise_perf = np.array(\n",
    "        df.groupby(\n",
    "            'consensus'\n",
    "        ).aggregate(\n",
    "            {'model_correct':'mean'}\n",
    "        )['model_correct']\n",
    "    )\n",
    "\n",
    "    n_models = 1\n",
    "    n_tests = 250\n",
    "    model_preds_dict_new = {\n",
    "        'model_confs' : model_confs[:,start_point:start_point+n_tests],\n",
    "        'model_preds' : model_preds[:,start_point:start_point+n_tests],\n",
    "        'targets' : d_new[start_point:start_point+n_tests],\n",
    "        'true_targets' : d_new[start_point:start_point+n_tests],\n",
    "        'expert_preds' : y_h[:,start_point:start_point+n_tests],\n",
    "        'chosen_models' : np.array([0]),\n",
    "        'model_perf' : model_perf,\n",
    "        'model_perf_per_class' : class_wise_perf\n",
    "    }\n",
    "\n",
    "    with open('cifar_infexp{}.pickle'.format(start_point), 'wb') as handle:\n",
    "        pickle.dump(model_preds_dict_new, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24ea3b4-843c-45ca-bb86-f226a7d9c1b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml] *",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
